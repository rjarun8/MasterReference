{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f0bf1450-a3e7-481f-ae60-f3e21b4f998e",
   "metadata": {},
   "source": [
    "__Important Hints:__\n",
    "\n",
    "\n",
    "Probability:\n",
    "    Frequentist and Bayesian\n",
    "    Bayesian:\n",
    "        Probabilistic graph models - captures conditional probabilities: Bayesian belief network, Bayesian Nets\n",
    "    Maximum Liklihood Estimation:\n",
    "        Maximizes the conditional probability of the data with a specific probability distribution and other parameters.\n",
    "        Computes the weights for observed data. OSL for linear regression and Expctation maximization(latent variables - Gaussian Mixture Model) for unsupervised. Probability class difference for classification and neural networks.\n",
    "    Marginal Probability : P(A),1-P(A)\n",
    "    Joint Probability p(A,B) = P(A) * P(B) == p(A int B)\n",
    "        if B is given joint probability is P(AnB) = P(A/B)*P(B)\n",
    "        P(A,B)=P(B,A)\n",
    "    \n",
    "    Conditional probability:\n",
    "        P(A/B)=P(A,B)/P(B)=P(B,A)/P(B)= P(B/A)*P(A)/P(B)\n",
    "    Degree of  belief  \n",
    "    Bayes Theorm - Liklihood , Posterior , Prior , evidence  \n",
    "    \n",
    "Distribution:\n",
    "    Bernouli --> Binomial\n",
    "    Multinouli --> Multinomial\n",
    "    PMF (discrete) and  PDF (continuous)\n",
    "    PMF and PDF can be used over a distribution to calculate the likelihood of an event\n",
    "    \n",
    "    Parametric Density Function:\n",
    "        the normal distribution has two parameters: the mean and the standard deviation. Given these two parameters, we now know the probability distribution function. These parameters can be estimated from data by calculating the sample mean and samplestandard deviation. We refer to this process as parametric density estimation. The reason is that we are using predefined functions to summarize the relationship between observations and their probability that can be controlled or configured with parameters, hence parametric.\n",
    "        \n",
    "    Nonparametric Density Estimation:\n",
    "        In some cases, a data sample may not resemble a common probability distribution or cannot\n",
    "be easily made to fit the distribution. This is often the case when the data has two peaks (bimodal distribution) or many peaks (multimodal distribution). In this case, parametric density estimation is not feasible and alternative methods can be used that do not use a common distribution. Instead, an algorithm is used to approximate the probability distribution of the data without a pre-defined distribution, referred to as a nonparametric method.\n",
    "\n",
    "        Kernl Density function:  A smoothed probabilistic distribution of the samples.The kernel function weights the contribution of\n",
    "observations from a data sample based on their relationship or distance to a given query sample\n",
    "for which the probability is requested . A parameter, called the smoothing parameter or the\n",
    "bandwidth, controls the scope, or window of observations, from the data sample that contributes\n",
    "to estimating the probability for a given sample. As such, kernel density estimation is sometimes\n",
    "referred to as a Parzen-Rosenblatt window, or simply a Parzen window, after the developers of\n",
    "the method.\n",
    "    \n",
    "    Bernouli - Binomial(Continuous bernouli trial) - normal - poisson  \n",
    "        _when the frequency distribution is normalized to its y-axis ,we get the relative frequency of the random variable and this random variable is either PMF or PDF .\n",
    "        \n",
    "Probability Density Estimation:\n",
    "    The function that best resolves a samples probability of availability in a domain that best describes its relationship with the target. We are dealing with joint probability case.\n",
    "    Methods used:\n",
    "        MAP - maximum a posteriori (Bayesian)\n",
    "        MLE - Maximum Liklihood\n",
    "        The difference between MAP and MLE is that MAP has prior information. But they converge and at convergence its assumed that the prior is also available for MLE. Now when the prior is assumed to have a uniform distribution , MAP and MLE equates\n",
    "    Probability Divergence:\n",
    "        KL Divergence:\n",
    "            One way to measure the dissimilarity of two probability distributions, p and q, is known as the Kullback-Leibler divergence (KL divergence) or relative entropy.\n",
    "        JS Divergence:\n",
    "            Normalizing or symmetry score of KL Divergence\n",
    "            \n",
    "    Cross Entropy:\n",
    "        the cross-entropy is the average number of bits needed to encode data coming from a source with distribution p when we use model q\n",
    "\n",
    "Cross-Entropy: Average number of total bits to represent an event from Q instead of P .\n",
    " Relative Entropy (KL Divergence): Average number of extra bits to represent an event from Q instead of P .\n",
    "\n",
    "\n",
    "Z-test for poppulation and sample mean - Hypothesis  \n",
    "T-Test Ind for K-Fold samples - Hypothesis  \n",
    "\n",
    "P < Alpha , reject null. P>alpha , fali to reject null.  \n",
    "\n",
    "Monte carlo simulation - to validate the  closeness to degree of belief\n",
    "\n",
    "1sample t-test:\n",
    "    test of mean within a distribution of a sample\n",
    "    \n",
    "ind_ttest:\n",
    "    test of mean between two sets of samples\n",
    "    \n",
    "Z-test:\n",
    "    when population mean and SD is known, evaluate with sample\n",
    "    \n",
    "Paired Test:\n",
    "    before and after distribution for a sample\n",
    "    Wilcoxon signed rank test used if the differences between pairs of data are non-normally distributed\n",
    "    \n",
    "Mann-Whitney U Test:\n",
    "    used to compare whether there is a difference in the dependent variable for two independent groups\n",
    "    \n",
    "Cohens's D:\n",
    "    Cohen's d is an effect size used to indicate the standardised difference between two means.\n",
    "\n",
    "Histogram is a representation of a dictionary\n",
    "\n",
    "Hosmer-Lemish test:\n",
    "    Goodness of fit for logistic regression\n",
    "\n",
    "PMF & Histogram:\n",
    "    Similar nature . Discrete values\n",
    "    \n",
    "PMF is discrete and PDF is continuous. PMF is for a unit. PDF is for the fraction of a unit.PDF is differentiaiton of PMF and PMF is integration of PDF. Continuous curves are smooth.\n",
    "\n",
    "Robust: A statistic is robust if its relatively immune to the effect of outliers\n",
    "\n",
    "Pearson Median Skweness:\n",
    "    Robust statistic\n",
    "    \n",
    "Correlation:\n",
    "    A statistic intended to quantify the strength of relationship between two variables\n",
    "    Pearson Product Moment Correlation co-eff. Transform each correlated variable to a standard score. For linera variables\n",
    "    Transform each value to its rank , Spearman rank correlation co-eff. For non linear variables\n",
    "    \n",
    "Covariance:\n",
    "    Tendency of 2 variables to vary together\n",
    "    \n",
    "    \n",
    "Causal Relationship:\n",
    "    A-->B\n",
    "    B-->A\n",
    "    X-->(A,B)\n",
    "    \n",
    "Estimator\n",
    "    Error Calculators\n",
    "\n",
    "Sampling Error:\n",
    "    Two types: Standard error & Standard Deviation\n",
    "        SE - Variability in an estimate\n",
    "        SD - variability in a measured quantity\n",
    "     Probalistic & Non-Probabilistic\n",
    "        \n",
    "    As sanple  size increases SE gets smaller and SD goes out\n",
    "    \n",
    "Statistical Power test:\n",
    "    Used when the null is false or rejected. It estimates the probability of the null hypothesis getting rejected\n",
    "    \n",
    "Tolerance interval:\n",
    "    decreases with increase in sample size\n",
    "    tolerance interval includes data coverage and confidence of coverage\n",
    "\n",
    "Confidence Interval:\n",
    "\n",
    "Prediction Interval:\n",
    "\n",
    "Three examples of statistical methods for normality testing,\n",
    "as it is called, are:\n",
    " Shapiro-Wilk Test.\n",
    " D’Agostino’s K 2 Test.\n",
    " Anderson-Darling Test.\n",
    "\n",
    "Four nonparametric statistical correlation methods that you can use are:\n",
    " Spearman’s Rank Correlation.\n",
    " Kendall’s Rank Correlation.\n",
    " Goodman and Kruskal’s Rank Correlation.\n",
    " Somers’ Rank Correlation.\n",
    "\n",
    "Four nonparametric statistical significance tests that you can use are:\n",
    " Mann-Whitney U Test.\n",
    " Wilcoxon Signed-Rank Test.\n",
    " Kruskal-Wallis H Test.\n",
    " Friedman Test.\n",
    "\n",
    "If Data Is Gaussian:\n",
    "    Use Parametric Statistical Methods\n",
    "Else:\n",
    "    Use Nonparametric Statistical Methods\n",
    "\n",
    "Information Gain & Entropy :\n",
    "    Inversely propoertional\n",
    "    \n",
    "Sensitivity :\n",
    "    TPR - Recall\n",
    "    \n",
    "Specificity:\n",
    "    TNR\n",
    "    \n",
    "Bayes Theorm:\n",
    "    Posterior(A/B) = Liklihood(B/A)* Prior(pA)/marginal liklihood(pB)\n",
    "    \n",
    "Parametric algorithms are sensitive to units, scales and outliers. Hence data transformation  is required.\n",
    "\n",
    "Feature Transformation:\n",
    "    For skewed data , log transform features such that it forms a normal distribution\n",
    "    Scaling is to be done for continuous variables\n",
    "\n",
    "Features Generation: \n",
    "    Supervised:\n",
    "        Binning\n",
    "        Encoding\n",
    "            Binary,target based, scalar,one hot,dummy(N-1)\n",
    "    Unsupervised\n",
    " \n",
    " \n",
    "Dimensionality reduction:\n",
    "    Feature selection & Extraction\n",
    "    Feature Selection:\n",
    "        Filter\n",
    "            Pearson correlation - for  linear\n",
    "            MIC - non linear\n",
    "            Anova\n",
    "            ChiSquare - Categorical\n",
    "            \n",
    "         \n",
    "        Wrapper\n",
    "            Model based automated features\n",
    "               Step wise regression:\n",
    "                   high VIF & high P value , we reject the  feature\n",
    "                   Forward,backward,standard\n",
    "               Recurssive Feature  Elimination\n",
    "               Stabiity Selection\n",
    "               \n",
    "        Embedded\n",
    "             L1 & L2.\n",
    "             L1 - Reduces co-eff to 0. The distance of the vector is calculated as the sum of the scalars\n",
    "             L2 - Generalizes coeff. The distance of the vector is calculated as the square root of the sum of the square of the scalars\n",
    "       \n",
    "       \n",
    "       \n",
    "     Feature Extraction:\n",
    "         PCA,LDA,Factor Analysis(EFA & CFA)\n",
    "         Takes care of multicolinearity which results is in data overfit\n",
    "         \n",
    "Type of  model  error:\n",
    "    Bias  error and variance error\n",
    "    \n",
    "Convex function: loss minization (minima)\n",
    "Concave function: parameter maximation (maxima)\n",
    "\n",
    "Eigen Vector: Representation of a linear distriution after undergoing a orthoganal transformation by a scalar factor\n",
    "Eigen Value:\n",
    "    The scalar factor that accompanies a eigen vector is a eigen value.\\\n",
    "Ex: When pressure is applied to a plastic sheet, it deforms in different directions which is the principle component and the direction of max deform is the eigen vector of that acconpanied by a scalar value which is the eigen value.\n",
    "\n",
    "Consider a 2-Dimensional data set, for which 2 eigenvectors (and their respective eigenvalues) are computed. The idea behind eigenvectors is to use the Covariance matrix to understand where in the data there is the most amount of variance. Since more variance in the data denotes more information about the data, eigenvectors are used to identify and compute Principal Components.\n",
    "\n",
    "Eigenvalues, on the other hand, simply denote the scalars of the respective eigenvectors. Therefore, eigenvectors and eigenvalues will compute the Principal Components of the data set.\n",
    "\n",
    "         \n",
    "         \n",
    "Linear Regression:\n",
    "    Linear Regression can be converted to logistic when the dependent variable is binary and the output of linear funtion\n",
    "is transformed by a logistic function\n",
    "    y=mx+b, y1 = y=mx+b+e\n",
    "    Hetroscedasticity\n",
    "        variance of residual is the  same for any value of x\n",
    "    Error is expected  to follow a normal distribution\n",
    "    Regularized linear regression:\n",
    "        reduces multicolinearlity & model complexity(overfit)\n",
    "    OLS struggles to generalized a bias dataset\n",
    "    Reduction in variance(Anova) is the end goal\n",
    "    Liner data - simple linear refression\n",
    "    Non - linear data - Polinomial regression(curved)\n",
    "    VIF - Variance Inflation Factor\n",
    "        Ratio of model variance to the individual feature variance of that model\n",
    "    AIC:\n",
    "        Goodness of fit.estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.AIC is mainly used to select among multiple models for a given dataset. A lesser value of AIC indicates that the model is of good quality. AIC tries to strike a balance between the variance and bias of the model. Therefore, it deals with the chances both of overfitting and underfitting. The model with the lowest AIC score is preferred over other models.\n",
    "    Frameworks for identifying the parameters to make the model best fit the data are :\n",
    "        MLE and LSO(seeks smaller squared error)\n",
    "        \n",
    "    \n",
    "    \n",
    "Decision making algorithms:\n",
    "    ID3,C4.5,C5 standards\n",
    "    ChiSquare,Entropy,Gini(binary split)\n",
    "    \n",
    "Spatial  Algorithms:\n",
    "    Classificaiton: Majority Voting. Kernel based (Weighted majority voting)\n",
    "    Regression: Mean values of  K\n",
    "    Distance:\n",
    "        Euclidean,Hamming(Categorical data),Minkowski(Generalized  form of euclidean)\n",
    "        KL Divergence - for discrete values\n",
    "        BM25 - Text  related problems\n",
    "    KNN techniques to approximate neighbours:\n",
    "        KD,Local Sensitivity hashing,inverted lists\n",
    "        \n",
    "        \n",
    "Ensemble:\n",
    "    Homogeneous and hetrogeneous\n",
    "    Bagging:\n",
    "        Averaging. Parallel\n",
    "    Boosting\n",
    "        Sequential. Weighted\n",
    "        Ada Boost: Uses caliberated weights for the classes to power up the prediction. Special  case algo\n",
    "        Gradient boost: reduces the loss iteratively by working on the gradients to reduce the loss. Generalized  algo.\n",
    "            XGB is a special case.  Supports parallel processing\n",
    "    Stacked\n",
    "        Ensemble of ensembles\n",
    "        \n",
    "epsilon:\n",
    "    eps value prevents division by zero errors when normalizing\n",
    "    \n",
    "    \n",
    "Probabilistic Model Selection:\n",
    "    C0mbines the complexity of the model with the performance to a score\n",
    "    Akaike Information Creterion:\n",
    "        the model with the lowest AIC is selected, the penalty for AIC is less than for BIC. This causes AIC to pick more complex models with higher performance.\n",
    "    Bayesian Information Criterion:\n",
    "        the model with the lowest BIC is selected.BIC penalizes the model more for its complexity.\n",
    "    Minimum Description Length:\n",
    "        the model with the lowest MDL is selected.\n",
    "    \n",
    "    \n",
    "    \n",
    "Bayes Theorem of Modeling Hypotheses:\n",
    "    max h ∈ HP (h|D) = P (D|h) -->P(D) and P(h) are removed since constant\n",
    "    Bayes Optimal classifier and Bayes error\n",
    "    Because of the computational cost of this optimal strategy, we instead can work with direct simplifications of the approach. Two of the most commonly used approaches are using a sampling algorithm for hypotheses such as Gibbs sampling, or to use the simplifying assumptions of the Naive Bayes classifier.\n",
    " Gibbs Algorithm. Randomly sample hypotheses biased on their posterior probability.\n",
    " Naive Bayes. Assume that variables in the input data are conditionally independent\n",
    "    Log Trick:\n",
    "        when the multuplication of many small numbers result in unstable results use the log trick with is to do a sum of the log probabilities\n",
    "     Bayesian Optimization:\n",
    "         Building a probabilstic model for the objectve function which is calld the surrogate function . This surrogate function is searched efficiently by an acquization function for optimal parameters\n",
    "         \n",
    " Information Theory:\n",
    "     A foundational concept from information theory is the uantification of the amount of information in things like events, random variables, and distributions. Quantifying the amount of information requires the use of probabilities, hence the relationship of information theory to\n",
    "probability. Measurements of information are widely used in artificial intelligence and machine learning, such as in the construction of decision trees and the optimization of classifier models. As such, there is an important relationship between information theory and machine learning and a practitioner must be familiar with some of the basic concepts from the field.\n",
    "\n",
    " Low Probability Event: High Information (surprising).\n",
    " High Probability Event: Low Information (unsurprising).\n",
    "    Shannon onformation: information or probability contained in an event\n",
    "    Rare event - Information\n",
    "    Random event - Entropy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05db9bb3-0b2e-4431-a42c-5199fdb2f150",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b770875-6452-48f0-bf36-f17e63fbc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ba8023d-33a3-4d87-b3d2-62fd91830571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas stats matplotlib scipy statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be03a828-e746-41d5-831e-a60d6162c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "import matplotlib.pyplot as plt\n",
    "month_names = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']\n",
    "months = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "raindata = [2,5,4,4,0,2,7,8,8,8,1,3]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecb29ce7-ac6a-4032-952d-1c1dd75a8c71",
   "metadata": {},
   "source": [
    "fig , ax = plt.subplots(nrows=1,ncols=1)\n",
    "ax.set_title('measure of center')\n",
    "ax.set_xlabel('month')\n",
    "ax.set_ylabel('number of  times it rained')\n",
    "\n",
    "ax.scatter(months,raindata)\n",
    "plt.xticks(np.arange(12)+1,month_names,color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "163e3f95-b4d9-4bcd-8a91-fc8b484db6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cards = 52\n",
    "num_aces =  4\n",
    "num_hearts = 13\n",
    "num_diamonds = 13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f1b716a-6638-476a-8fd1-3d69e68229eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "prob_ace = num_aces/num_cards\n",
    "print(prob_ace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e077fb2-b0b5-460f-99d7-5c8940e72fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "prob_red_card = (num_hearts+num_diamonds)/num_cards\n",
    "print(prob_red_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7060fd48-0107-4a5d-adb9-be190a8f095e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rem_cards --> 51\n"
     ]
    }
   ],
   "source": [
    "total_card = 52\n",
    "cards_drawn = 1\n",
    "rem_cards = total_card - cards_drawn\n",
    "print('rem_cards -->', rem_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3488f0a8-398b-45ba-8f52-7fe3773b1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queens = 4\n",
    "num_kings = 4\n",
    "prob_king1 = num_kings/total_card\n",
    "prob_queen1 = num_queens/total_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8f08991-fb02-4817-b95f-ffeaa07bc821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint probability --> 0.6\n"
     ]
    }
   ],
   "source": [
    "prob_king_queen = prob_king1 * prob_queen1\n",
    "print('joint probability -->', round(prob_king_queen,3)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7396288-1665-401e-bd6f-57f684cb163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_king2 = num_kings/rem_cards\n",
    "prob_queen2 = num_queens/total_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7d47176-809b-490d-bfea-52aacaf93e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint probability --> 0.6\n"
     ]
    }
   ],
   "source": [
    "prob_king2_queen2 = prob_king2 * prob_queen2\n",
    "print('joint probability -->', round(prob_king2_queen2,3)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ac24140-66f8-48fd-adfa-f8b0307fadb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31917526, 0.76505268, 0.56469922, 0.15682046, 0.30204696,\n",
       "       0.83981842, 0.78609758, 0.50798807, 0.16887905, 0.32241064])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1127f75c-8963-4ad9-b0e4-134f2a258ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.44654673850829"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52067d2a-f1cf-4fbd-9d6e-60ed3b9c7152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98ecc0b8b0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(random.uniform(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb75dd88-3a6f-49a6-87d6-d04504701c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f07aa19-cd97-465b-9d38-a68cb0f6a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reject null\n",
      "1.959963984540054 -1.959963984540054\n"
     ]
    }
   ],
   "source": [
    "'''#Z-Test example Code  Snippet#'''''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import math\n",
    "import scipy.stats as st\n",
    "population = np.array([20,21,32,43,54,54,65,76,87,65,87,65,43,21,13,14,16,17,18,24,25,27,2,86,64,53,31,87,64,34,65,98,76,34,65,23])\n",
    "sample_size=20\n",
    "sample = random.choice(population,sample_size)\n",
    "\n",
    "\n",
    "population_mean =  population.mean()\n",
    "sample_mean = sample.mean()\n",
    "pop_std = population.std()\n",
    "z_test = (sample_mean - population_mean)/(pop_std /math.sqrt(sample_size))\n",
    "\n",
    "if (z_test >0):\n",
    "    p_val = 1 - st.norm.cdf(z_test)\n",
    "else:\n",
    "    p_val = st.norm.cdf(z_test)\n",
    "    \n",
    "confidence_level = .95\n",
    "alpha = 1- confidence_level\n",
    "if (p_val < (alpha/2)):#if one tail alpha/2 else  alpha\n",
    "    print('accept  null')\n",
    "else:\n",
    "    print('reject null')\n",
    "    \n",
    "z_critical_val1 = st.norm.ppf(confidence_level+alpha/2)#right_tail\n",
    "z_critical_val2 = st.norm.ppf(alpha/2)#left_tail\n",
    "\n",
    "print(z_critical_val1,z_critical_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1027cbf1-af2b-4cf8-868d-bad9a77137cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de44449f-5118-481c-a8a2-de25828f67aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff23c977-26f0-41ce-9a3c-3cf6ed58ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reject null\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e109952d-66c0-4b19-92b3-63a128b73a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cabc393-1df9-436d-bf61-e58d1577b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.959963984540054 -1.959963984540054\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c59f72-8e32-4e22-8155-b25bb47587a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
