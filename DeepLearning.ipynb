{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a714ee00-2f6e-43ce-9e54-b22b0a13685d",
   "metadata": {},
   "source": [
    "'''\n",
    "Deep Learning:\n",
    "\n",
    "Perceptron:\n",
    "    sum(wx+b)*(activation/transfer function)\n",
    "\n",
    "Activation Functions:\n",
    "    Sigmoid,tanh,Relu,Logistic,LeakyRelu - solves vanishing gradient.\n",
    "    nonlinear behaviour of the activation function gives the deep nets to learn complex functions\n",
    "    Differentials  functions are preferred\n",
    "    \n",
    "    Sigmoid - has vanishing gradient issues\n",
    "    tanh - has fewer vanishing gradient issues\n",
    "    (TanH and sigmoid fire at all times)\n",
    "    ReLU: makes the positive ones pass and negatives stale hence making it selectively firing and hence a good choice\n",
    "    \n",
    "Cost/Loss Function:\n",
    "    Softmax:\n",
    "        Softmax is a kind of activation function with the speciality of output summing to 1. It converts the outputs \n",
    "        to probabilities by dividing the output by summation of all the other values. The Euclidean distance can be computed \n",
    "        between softmax probabilities and one-hot encoding for optimization.\n",
    "    Cross-Entropy:\n",
    "        Cross-entropy is the summation of negative logarithmic probabilities. \n",
    "    L1 - Penalizes absolute value of weights to  zero\n",
    "    L2 - Penalizes squared value of weights\n",
    "        \n",
    "Regularization:(To reduce  overfitting)\n",
    "    Dropout,Batch Normalization - zero mean and one Std Dev, \n",
    "    \n",
    "Forward Propagation:\n",
    "    Weights are randomly initialized to the features\n",
    "    \n",
    "Backward Propagation:\n",
    "    procedure of updating the weights is called backpropagation\n",
    "    Using loss reduction algorithms , the weights are updated based on the loss from a forward propagation\n",
    "    Loss is minimized  using solver functions which is the loss minimizing the algorithm. The function is expected to be a convex solver so that \n",
    "    the local minima is also the global minima\n",
    "    Types: LBFGS, Gradient Descent(full full data set) - SGD or Minibatch(for every sample)\n",
    "    SGD works better in practice for optimizing non-convex cost functions\n",
    "    \n",
    "    Learning Rate:\n",
    "        Optimizers(learning Rate):\n",
    "            Adam, Momemtum,RMSProp\n",
    "            \n",
    "Optimization:\n",
    "    procedure to minimize the error is called optimization\n",
    "    when you change the scale of the weights ,Optimizer like SGD whose learning rate is dependent of the magnitude of the gradient. Where as ADAM is unaffected by the scaling change\n",
    "    \n",
    "Flow:\n",
    "    Loss & Optimizer are the driving components in na NN\n",
    "    Loss gets input from a loss calculator of input function and actual value\n",
    "    Optimizer get the the input from the loss and uses a minimizing / solver algorithm to  reduce the loss.  As the loss is\n",
    "    reduced , the weights are accordingly updated\n",
    "    The weights is a Random normal  distriubution of input_size * class\n",
    "    the bias is a Random normal distribution of class\n",
    "    Input function / logit is a matmul(x*w) + b\n",
    "    \n",
    "Logits:\n",
    "    raw output of the model which is a probability distribution(PMF). this is de-normalized which is then passed to a normalization fn such as sigmoid or softmax. comes in multiclass classification\n",
    "    \n",
    "Model Capacity\n",
    "    number of parameters in a model\n",
    "    \n",
    "Learning rate\n",
    "    As you train the model it is a good practise to reduce the learning rate over time. Use learningrate scheduled and pass it as a paramenter to \n",
    "    optimizer\n",
    "\n",
    "Notes:\n",
    "universal approximation theorem suggests that such a neural network can approximate any function\n",
    "Tensorflow Playground\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################################################    \n",
    "###################################################################################################################################################    \n",
    "###################################################################################################################################################    \n",
    "###################################################################################################################################################    \n",
    "###################################################################################################################################################    \n",
    "    \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efd7662b-25cc-424c-95cf-eb069ba43ecd",
   "metadata": {},
   "source": [
    "'''\n",
    "CNN:\n",
    "    neurons of a CNN are arranged in a volumetric fashion to take advantage of the volume. \n",
    "    Each of the layers transforms the input volume to an output volume\n",
    "    Original dataset (e.g. train_data) -> map() -> shuffle() -> batch() -> prefetch() -> PrefetchDataset\n",
    "    data-->augumentation model layer-->base model(trainable false, include top false)-->fine tuning(trainable true)-->mixed precision(policy)--> earlystopping,model checkpoint, ReduceLRonPlateau (callbacks)\n",
    "    float32 - 32 bit precision, float16 - 16 bit precision\n",
    "    Convergence: Model getting close to its ideal performance\n",
    "    you can either load the model  architecture or weights or both\n",
    "\n",
    "    \n",
    "    Kernel:\n",
    "        also  a filter. Weights matrix. It has 2 params: stride & size\n",
    "        \n",
    "    Max Pooling:\n",
    "        Sampler. Reduces the dimension of the image  by extracting most important features only.\n",
    "        \n",
    "    Common Use Cases:\n",
    "        Classification, Localization, detection , segmentation , \n",
    "        Image captioning:\n",
    "            Image captioning is the task of describing the image with text \n",
    "        Similarity learning:\n",
    "            Similarity learning is the process of learning how two images are similar\n",
    "            \n",
    "NLP:\n",
    "    Tokenization: word and character\n",
    "    Embedding: learned representation of the documents. two types --> custom & pretrained\n",
    "                TOken vs character\n",
    "    The weights and metadata of embedders can be extracted and visualized usinf PCA,t-SNE and other dimensionality redection algorithms\n",
    "        \n",
    "    RNN:\n",
    "       Sequence data. Current output is based on previous weights computed (t-1).\n",
    "       It cannot  remember for longer sequences or time hence rises vanishing gradient. LSTM overcomes this.\n",
    "       information from the past to help with the future(recurrent)\n",
    "       Applications: One to one , one to many(image caption), many to one (classification), many to many(audio as in , text as out)\n",
    "       Types: LSTM, GRU, Bi-directional\n",
    "       Bidirectional wrapper can be applied to GRU and LSTM\n",
    "\n",
    "    LSTM:\n",
    "        Input gate , output gate and forget gate.\n",
    "        If the state of the output gate qualifies to pass then the forget gate is released. Else the state is retained.This makes \n",
    "        it to remember longer sequences and time thereby updating the  gradients.\n",
    "        \n",
    "        \n",
    "    Conv1D - also called as temporal model\n",
    "    \n",
    "    \n",
    "Transferlearning:\n",
    "    Ensure the shape of the image matches the transfer architecture to be used\n",
    "    the normalization should also match [0,1] or [-1,1]\n",
    "    bottle neck layer: the layer just before the flatten where feature are well generalized(include top = false). Feature Extraction\n",
    "    Batch normalization layers should be explicitly set to trainable : false. When the model is in freeze mode, batch nomalization will be inference mode.\n",
    "    when un freezing the model , the batch normalization trainable should be set to false and retraining of the batch normaization layer will destroy\n",
    "    wverything the model has learned\n",
    "    Used for feature extraction and fine-tuning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6be3c2c-3a1d-4609-8dda-3932169433ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPyimageSearch:\\n\\n\\n\\nClustering based on pixel intensity using KMEANS\\n\\nHistogram comparison between images to identify close and far images using distance metric\\n\\n    For some similarity functions a LARGER value indicates higher similarity (Correlation and \\n    Intersection). And for others, a SMALLER value indicates higher similarity (Chi-Squared and \\n    Hellinger).\\n\\n\\nPytorch:\\n    Defining your neural network architecture - Initializing your optimizer and loss function - \\n    Looping over your number of training epochs - Looping over data batches inside each epoch - \\n    Making predictions and computing the loss on the current batch of data - Zeroing out your gradient - \\n    Performing backpropagation - Telling your optimizer to update the gradients of your network - \\n    Telling PyTorch to train your network with a GPU\\n    *The output dimension of (t-1) layer should match with the input dimension of t layer\\n    * train mode for training and eval mode along with no_grad context for validation\\n    *automatic differentiation - compute derivative from the loss and use optimizer to update weights\\n    *when we comine NNLOSS with softmax it becomes Linear combined with cross entropy categorical classification\\n    layer\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PyimageSearch:\n",
    "\n",
    "Neural Network are stochastic algorithms as there is randomness implied on the object or target function\n",
    "\n",
    "\n",
    "\n",
    "Clustering based on pixel intensity using KMEANS\n",
    "\n",
    "Histogram comparison between images to identify close and far images using distance metric\n",
    "\n",
    "    For some similarity functions a LARGER value indicates higher similarity (Correlation and \n",
    "    Intersection). And for others, a SMALLER value indicates higher similarity (Chi-Squared and \n",
    "    Hellinger).\n",
    "\n",
    "\n",
    "Pytorch:\n",
    "    Defining your neural network architecture - Initializing your optimizer and loss function - \n",
    "    Looping over your number of training epochs - Looping over data batches inside each epoch - \n",
    "    Making predictions and computing the loss on the current batch of data - Zeroing out your gradient - \n",
    "    Performing backpropagation - Telling your optimizer to update the gradients of your network - \n",
    "    Telling PyTorch to train your network with a GPU\n",
    "    *The output dimension of (t-1) layer should match with the input dimension of t layer\n",
    "    * train mode for training and eval mode along with no_grad context for validation\n",
    "    *automatic differentiation - compute derivative from the loss and use optimizer to update weights\n",
    "    *when we comine NNLOSS with softmax it becomes Linear combined with cross entropy categorical classification\n",
    "    layer\n",
    "    \n",
    "    \n",
    "LBP - Local Binary patterns:\n",
    "    center pixel thresholded with neighbourhood. \n",
    "    p - number points, r - radius of circle\n",
    "    Uniform patterns - (0,1) & (1,0). at most 2 patterns\n",
    "    for p points, P+1 uniform patterns and histogram p+2\n",
    "\n",
    "Keras Tuners:\n",
    "    Bayesian Optimization\n",
    "    \n",
    "@tf.function:\n",
    "    Convert function to tensorflow graph\n",
    "    Use tensorflow operations where ever possible\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04dbb207-bb39-4d05-94bf-e82286d302a3",
   "metadata": {},
   "source": [
    "Federated Learning:\n",
    "    Global/Central model-Node Models-\n",
    "    Federated Learning Types:\n",
    "        Vertical & Horizontal, Transfer\n",
    "        data_format={client:{data,label}}\n",
    "        global model -> send to client -> client gets weights from global model -> client trains the model with client data -> if model accuracy increases , new weights are updated and sent to server , else only ack is sent -> the server received weights from all the clients and performs a weighted average of the weights received by comparison of the better weights with t-1 instance. The poor weights are not considered for weighted average in this case.\n",
    "    Privacy Mechanism:\n",
    "        Secured Aggregation: \n",
    "            private keys are used to consumer the parameters flowing from different models.\n",
    "            Homo-morphic encription allows mathematical computation on the encrypted data. Using public keys data is encrypted and using private keys data is decrypted.        \n",
    "            Differential Privacy:\n",
    "                measured by using epsilon. ε = 0, then you have an exact distribution and you have achieved peak privacy. f(Dn) represents the data function and f(D’n) represents the data function after adding the noise.\n",
    "                Unique differential privacy is made up of threshold and noise. Noise will have drastic impact on small data set/rare classes and minimalistic impact on large datasets. A threshold will help determine the value of the noise\n",
    "                Tensorflow Differential Privacy:\n",
    "                    It limits the influence of single datapoint  in the resulting gradient\n",
    "                    The derivative of gradient descent are clipped making it agnostic.\n",
    "                    Performance is measured by privacy budget and epsilon\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f37a4d-85af-457f-9bc8-f694a680a9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e3a1d-7d5f-4521-b699-1fcc56ddb90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7270446-23dc-47ba-98b2-cf7dd1fca5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3bcd3e-6fb5-48d5-8f9b-1f6afe51612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd353280-8e0a-4729-926b-400973e98778",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://127.0.0.1:5000//api_predict\"\n",
    "\n",
    "data = {\"slen\":5,\"swid\":10,\"plen\":20,\"pwid\":30}\n",
    "\n",
    "r = requests.post(url,json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1fb21-10a5-4192-8110-c837ea9b508b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
